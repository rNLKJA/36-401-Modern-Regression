---
title: "36-401, Chapter 1: Review of Random
Variables"
output: html_notebook
---

## Motivation

Regression involves learning the relationship (if any) between outcomes Y and covariates X. Given n observations, our data will look like $(X_1, Y_1), \dots, (X_n, Y_n)$, where each Xi and Yi is organized by columns in a dataset. Example 0.1. The US Bureau of Economic Analysis (BEA) releases data on the economic output of metropolitan areas. Below we consider data from 2006, where we assess the relationship between per-capita gross metropolitan product (GMP) (the outcome) and population size (the covariate). 

We can load the data and look at the first few rows:

```{r}
bea_url <- "https://www.stat.cmu.edu/~cshalizi/TALR/data/bea-2006.csv"
bea <- read.csv(bea_url, na.strings = c("NA", "", "."))

head(bea)
```

We’ll visualization pcgmp, pop, and their relationship. First, we can visualize the variables’ marginal distributions.

```{r}
library(ggplot2)
library(gridExtra)

pcgmpHist <- ggplot(bea, aes(x = pcgmp)) +
geom_histogram(binwidth = 2000) +
labs(x = "Per capita GMP", y = "MSAs")
popHist <- ggplot(bea, aes(x = pop)) +
geom_histogram(binwidth = 500000) +
labs(x = "Population", y = "MSAs")
grid.arrange(pcgmpHist, popHist, ncol = 2)
```

Now, we can visualize the variables’ joint distribution and their linear relationship with a scatterplot and linear regression line. Below we use pop as a covariate (left) or log(pop) as a covariate (right).

```{r}
#population as covariate
popScatter <- ggplot(bea, aes(x = pop, y = pcgmp)) +
geom_point() +
geom_smooth(method = "lm", formula = y ~ x) + 
labs(x = "Population", y = "Per-capita GMP")

#log(population) as covariate
logPopScatter <- ggplot(bea, aes(x = pop, y = pcgmp)) +
geom_point() + geom_smooth(method = "lm", formula = y ~ log(x)) +
scale_x_continuous(trans = "log") +
labs(x = "Log Population", y = "Per-capita GMP")
grid.arrange(popScatter, logPopScatter, ncol = 2)
```

There are several key takeaways from the above example.

### Key takeaways

- Skewed predictors: Population is extremely right‑skewed; a raw linear fit $\text{pcgmp} = \beta_0 + \beta_1 \text{pop} + \varepsilon$ is dominated by a few huge cities.  
- Scale transformation helps: Using $\log(\text{pop})$ compresses the scale, reduces leverage, and often reveals a clearer, more proportional association.  
- Different interpretations: In the raw model, $\beta_1$ is the (tiny) dollar change in per‑capita GMP per extra person. In the log model $\text{pcgmp} = \alpha_0 + \alpha_1 \log(\text{pop}) + \varepsilon$, $\alpha_1 \log 2$ is the expected change in per‑capita GMP for a doubling of population.  
- Visual clarity: The log model typically produces a tighter, more linear cloud; the raw plot usually shows a fan shape and sparse right tail.  
- Potential heteroskedasticity: Spread in pcgmp often grows with population size; check residual plots and consider robust (heteroskedasticity‑consistent) standard errors.  
- Possible next refinement: Also logging the response, $\log(\text{pcgmp})$, would let you estimate an elasticity (percentage change in per‑capita GMP per 1% change in population).  
- Substantive reading: Larger cities appear to have systematically different productivity levels, but you’d need to control for industry mix and other covariates before attributing causality.  
- Practical habit: Always distinguish between (a) transforming the scale of axes and (b) transforming variables in the model formula—here you did the latter correctly with $\text{formula} = y \sim \log(x)$.  


## Distribution of Random Variables

A discrete random variable X is characterized by its probability mass function (pmf), denoted $f(·)$, where
$$f (k) = P(X= k), \forall k$$

Here, $k$ corresponds to different potential values of $X$.

**Exercise 0.1. List some properties that the pmf function must possess.**

For a discrete random variable $X$ with pmf $f(k) = P(X = k)$ :

1. Non-negativity: $f(k) \ge 0$ for all $k \in S$.
2. Normalization: $\sum_{k \in S} f(k) = 1$.
3. Probability of any event: For any subset $A \subseteq S$, $P(X \in A) = \sum_{k \in A} f(k)$.
4. Support definition: $f(k) = 0$ for $k \notin S$.
5. CDF connection: $F(x) = P(X \le x) = \sum_{k \le x} f(k)$.

A continuous random variable X is characterized by its probability density function (pdf), denoted $f(x)$, such that 
$$P(a \le X \le b) = \int^b_a f(x) dx,  \forall a \le b$$.

Example 0.2. A random variable X is said to have Exponential($\lambda$) distribution if:

$$
f(x) = \begin{cases}
\lambda e^{-\lambda x} & \text{if }x\ge 0 \\
0 & \text{if } x < 0
\end{cases}
$$

**Exercise 0.2. Suppose $X \sim \exp(\lambda)$. What is $P(X\le a)$? What is $P(3\le X \le 5)$? What is $P(X=2)$?**

1. $P(X \le a) = \begin{cases} 0, & a < 0 \\ 1 - e^{-\lambda a}, & a \ge 0 \end{cases}$
2. $P(3 \le X \le 5) = F(5) - F(3) = (1 - e^{-\lambda 5}) - (1 - e^{-\lambda 3}) = e^{-3\lambda} - e^{-5\lambda}$
3. $P(X = 2) = 0$ (since $X$ is continuous).

## Expected Values

The expected value of a random variable X is defined as

$E(X) = \sum_k f (k)$ when $X$ is discrete, and

$E(X) = \int^\inf_{-\inf}x f (x) dx$ when $X$ is continuous.

Heuristically, E[X] is the “average” value a random variable X takes. Often, expectations are denoted with µ.

The variance of X is defined as $$Var(X) = E (X− E[X])^2 = E[X^2]− (E[X])^2$$

The variance measures the spread of a distribution, often denoted with $\sigma^2$. The square root of the variance is the standard deviation.

The covariance between X and Y is defined as
$$Cov(X, Y) = E[(X− E(X)) (Y− E(Y))] = E(XY)− E(X) E(Y)$$

Covariance measures the strength of linear relationship between X and Y. A related quantity is the correlation between X and Y, defined as $$Corr(X, Y) = \cfrac{Cov(X, Y)}{\sigma_X\sigma_Y}$$
The correlation is bounded between −1 and 1.

There are several important properties of expectations, variances, and co-
variances that we’ll use throughout this class.

**Exercise 0.3. What is Cov(X, X)?**

$\operatorname{Cov}(X, X) = E\big[(X - E[X])(X - E[X])\big] = E\big[(X - E[X])^2\big] = \operatorname{Var}(X)$

Equivalent form:

$\operatorname{Cov}(X, X) = E[X^2] - (E[X])^2$

If $\operatorname{Var}(X) > 0$, then the correlation of $X$ with itself is

$\operatorname{Corr}(X, X) = \frac{\operatorname{Cov}(X, X)}{\sigma_X \sigma_X} = \frac{\operatorname{Var}(X)}{\sigma_X^2} = 1$

If $X$ is almost surely constant, then $\operatorname{Var}(X) = 0$ and $\operatorname{Cov}(X, X)=0$ (its correlation with itself is undefined because of division by zero).


The above expectations, variances, covariances, and correlations are population level quantities that we’ll estimate with sample analogs from the data.

```{r, echo=FALSE, fig.cap="A caption", out.width = '100%'}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/d/d4/Correlation_examples2.svg")
```


Figure : Examples of scatter plots, and corresponding correlations r. From Wikipedia. It shows some example scatterplots and corresponding correlations. It is useful for building intuition about correlation values.

## Conditional Expectation

Conditional expectations let us ask: What is the mean of $Y$ among observations where $X= x$? The conditional expectation of $Y$ given $X= x$ is

$$ E(Y|X= x) = \begin{cases} \sum_y y f (y|x) & \text{discrete case} \\ \int y f (y|x)dy & \text{continuous case}\end{cases}$$
This is the same definition of expectation, but we replaced the marginal $f_Y(y)$ with the conditional $f_{Y|X} (y|x)$.

Similarly, if $r(x, y)$ is a function of x and y, then $$E(r(X, Y)|X= x) = \begin{cases}\sum_y r(x, y) f (y|x) & \text{discrete case} \\ r(x, y) f (y|x)dy & \text{continuous case} \end{cases}$$

Warning! Whereas $E(Y)$ is a number, $E(Y|X= x)$ is a function of $x$. In fact, $E(Y|X)$ is a random variable whose value is $E(Y|X= x)$ when $X=x$.

**Exercise 0.4. Suppose we draw $X \sim \text{Unif}(0, 1)$. After we observe $X= x$, we draw $Y | X= x \sim Unif(x, 1)$. What is $E(Y | X)$?**

We have $X \sim \text{Unif}(0,1)$ and, conditional on $X = x$, $Y \mid X = x \sim \text{Unif}(x, 1)$ for $0 \le x \le 1$.

For a uniform distribution on $[a,b]$, the mean is $(a + b)/2$.  
Thus, for fixed $x$:

$$
E(Y \mid X = x) = \frac{x + 1}{2}.
$$

Therefore the conditional expectation as a random variable is

$$
E(Y \mid X) = \frac{X + 1}{2}.
$$

(Optional) Using the law of iterated expectations:

$$
E(Y) = E\big(E(Y \mid X)\big) = E\left(\frac{X+1}{2}\right) = \frac{E(X) + 1}{2} = \frac{\tfrac{1}{2} + 1}{2} = \frac{3/2}{2} = \frac{3}{4}.
$$

Summary:

$$
E(Y \mid X) = \frac{X + 1}{2}.
$$


The conditional variance of $Y$ given $X= x$ is $$Var(Y|X= x) = (y− µ(x))^2 f (y|x)dy$$ where $\mu(x) = E(Y|X= x)$.

Again, the conditional variance is a function of x and a random variable: It is the variance of $Y$ when (by chance) $X= x$.

Two very important properties of conditional expectations and variances:

**Exercise 0.5. Return to the above example. What is $E(Y)$?**

Setup recap:  
$X \sim \text{Unif}(0,1), \quad Y \mid X = x \sim \text{Unif}(x, 1), \; 0 \le x \le 1.$  
From Exercise 0.4 we found $E(Y \mid X = x) = \frac{x+1}{2}.$

We want the unconditional expectation $E(Y)$.

Method 1 (Law of Iterated Expectations):

$$
E(Y) = E\big(E(Y \mid X)\big) = E\left(\frac{X+1}{2}\right) = \frac{E(X)+1}{2}.
$$

Since $X \sim \text{Unif}(0,1)$, $E(X) = \frac{1}{2}$. Hence

$$
E(Y) = \frac{\tfrac{1}{2} + 1}{2} = \frac{\tfrac{3}{2}}{2} = \frac{3}{4}.
$$

Method 2 (Direct integration via joint density):

The conditional density is 

$$
f_{Y|X}(y|x) = \begin{cases}
\frac{1}{1 - x}, & x \le y \le 1 \\
0, & \text{otherwise}
\end{cases}
$$

and the marginal of $X$ is 
$$
f_X(x) = 1,\; 0 \le x \le 1.
$$

Joint density:
$$
f_{X,Y}(x,y) = f_{Y|X}(y|x) f_X(x) = \frac{1}{1 - x}, \quad 0 \le x \le y \le 1.
$$

Then
$$
\begin{aligned}
E(Y) &= \int_{0}^{1} \int_{x}^{1} y \,\frac{1}{1 - x} \, dy \, dx \\
&= \int_{0}^{1} \frac{1}{1 - x} \left[ \frac{y^{2}}{2} \right]_{y = x}^{y = 1} dx \\
&= \int_{0}^{1} \frac{1 - x^{2}}{2(1 - x)} \, dx \\
&= \int_{0}^{1} \frac{(1 - x)(1 + x)}{2(1 - x)} \, dx \\
&= \int_{0}^{1} \frac{1 + x}{2} \, dx \\
&= \left[ \frac{x}{2} + \frac{x^{2}}{4} \right]_{0}^{1} \\
&= \frac{1}{2} + \frac{1}{4} \\
&= \frac{3}{4}.
\end{aligned}
$$

Note: The cancellation $(1 - x^{2})/(1 - x) = 1 + x$ is valid for $x \ne 1$; the integrand has a removable discontinuity at $x = 1$, which does not affect the value of the integral.

Answer:

$$
E(Y) = \frac{3}{4}.
$$


## Large-Sample Theorems

In this class, we’ll consider the asymptotic (i.e., large-sample) behavior of estimators in terms of their bias and variance. The below two foundational results establish the asymptotic behavior of the sample mean X as an estimator for $E[X]$.

The Law of Large Numbers. Assume $(X_1,\dots, X_n)$ are independent and identically distributed (iid), where $E[X_i] < \inf$. Then $$\frac{1}{n}\sum^n_{i=1}X_i \rightarrow^p E(X)$$.

The $\rightarrow^p$  means “convergence in probability.” Informally, this means that the bias and variance of $X$ go to zero as $n \rightarrow \inf$. Formally, this means $$\lim_{n \rightarrow \inf} P(|\bar X− E[X]| > \epsilon) = 0, \forall \epsilon > 0$$

Thus, the sample mean is a consistent estimator for the population mean, which is reassuring. Central Limit Theorem. Assume $(X_1,\dots, X_n)$ are iid, where $E[X_i] < \inf$ and $Var(X_i) < \inf$. Then, as $n \rightarrow \inf$,

$$
\frac{1}{n}\sum^n_{i=1}X_i\sim N\bigg(E[X], \frac{Var(X)}{n}\bigg)
$$

Mathematically, it is nicer to have the limit that we’re converging to not change with n. Thus, the CLT is often stated as $$\sqrt{n}\bigg(\frac{\bar X - E[X]}{\sqrt{Var(X)}}\bigg) \rightarrow^d N(0,1)$$


The $\rightarrow^d$ means “convergence in distribution.” The CLT tells us that not only is $X$ an unbiased estimator, but also it has an asymptotically Normal distribution with a defined variance. If we can estimate this variance, then the distribution provides a way to compute confidence intervals.

```{r , echo=FALSE, fig.cap="A caption", out.width = '100%'}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/500px-Normal_Distribution_PDF.svg.png")
```

Figure 0.3: Three Normal densities.

## The Normal Distribution

The Normal distribution has the classic bell-shaped density that we have learned to love, shown in Figure 0.3: $$f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2} (\frac{x-\mu}{\sigma})^2}, -\inf < x < \inf$$

Writing $X\sim N(\mu, \sigma^2)$, implies that $E(X) = \mu$ and $Var(X) = \sigma^2$. The case where $\mu= 0$ and $\sigma 2 = 1$ is called the standard Normal.

**Exercise 0.6. Suppose that $X_1, X_2,\dots,X_n$ are each Normally distributed, i.e., $X_i ∼ N(\mu_i, \sigma^2_i)$. Under what condition(s) is the linear combination $$Y = \sum^n_{i=1}a_iX_i$$ (where at least one $a_i \ne 0$) also Normal?**

$Y$ is (univariate) Normal for all coefficient vectors $(a_1,\dots,a_n)$ iff $(X_1,\dots,X_n)$ is jointly (multivariate) Normal.

Common sufficient condition: If the $X_i$ are (mutually) independent (each marginal Normal), then they are jointly multivariate Normal, hence every linear combination (in particular $Y$) is Normal with

$$
E(Y)=\sum_{i=1}^n a_i \mu_i, \qquad
\operatorname{Var}(Y)=\sum_{i=1}^n a_i^{2}\sigma_i^{2}.
$$

More general case (allowing dependence):
Even if the $X_i$ are dependent, as long as the vector $X=(X_1,\dots,X_n)$ has a multivariate Normal distribution (i.e., there exists a mean vector $\mu$ and covariance matrix $\Sigma$ such that $X \sim N_n(\mu,\Sigma)$), then any linear combination $Y = a^\top X$ is Normal with

$$
E(Y)= a^\top \mu,\qquad \operatorname{Var}(Y)= a^\top \Sigma a.
$$

Why marginal Normality alone is not enough:
It is possible for each $X_i$ to be marginally Normal, yet the joint vector is not multivariate Normal; then some linear combinations need not be Normal.

Illustrative counterexample (showing necessity of joint Normality):
Let $Z \sim N(0,1)$ and $U$ be independent of $Z$ with $\Pr(U=1)=\Pr(U=-1)=\tfrac{1}{2}$. Define

$$
X_1 = Z,\qquad X_2 = U Z.
$$

Then each marginal is $N(0,1)$, and $\operatorname{Cov}(X_1,X_2)=E[U Z^{2}]=0$. But

$$
X_1 + X_2 = (1+U)Z =
\begin{cases}
2Z, & U=1 \ (\text{prob }1/2),\\
0, & U=-1 \ (\text{prob }1/2),
\end{cases}
$$

which is a 50–50 mixture of a point mass at 0 and $N(0,4)$ — not Normal. Thus mere marginal Normality (even with zero covariance) does not guarantee the linear combination is Normal.

Summary:
- Sufficient (and essentially necessary) condition: The vector $(X_1,\dots,X_n)$ is jointly multivariate Normal (independence is a common special case).
- Under that condition, every nontrivial linear combination is Normal.


```{r , echo=FALSE, fig.cap="A caption", out.width = '100%'}
knitr::include_graphics("https://saylordotorg.github.io/text_introductory-statistics/section_15/5a0c7bbacb4242555e8a85c9767c03ee.jpg")
```


Figure 0.4: Three chi-squared densities.

## Other Important Distributions
### Chi-Squared Distribution

If $Z_1, Z_2,\dots, Z_n\sim N(0, 1)$, then $$X = \sum^n_{i=1}Z_i^2 \sim \chi^2_n$$

i.e., the above sum follows a chi-squared distribution with n degrees of freedom.

```{r , echo=FALSE, fig.cap="A caption", out.width = '100%'}
knitr::include_graphics("https://cdn1.slideserve.com/2862804/slide1-n.jpg")
```


### The t-Distribution
If $Y \sim N(0, 1)$ and $U∼ \chi^2_n$ independent of $Y$, then $$X = Y / \sqrt{\frac{U}{n}} \sim t_n$$

i.e., the above quantity follows a t-distribution with n degrees of freedom. This distribution has a Normal-like shape, but with heavier tails.

```{r , echo=FALSE, fig.cap="A caption", out.width = '100%'}
knitr::include_graphics("https://sixsigmastudyguide.com/wp-content/uploads/2020/01/f14.png")
```

### The F-distribution
If $X∼ \chi^2_n$, and $Y \sim \chi^2_m$ independent of $X$, then $$U=\frac{X/n}{Y/m} \sim F_{n,m}$$

i.e., the above quantity follows an F-distribution with $n$ numerator and $m$ denominator degrees of freedom.
This distribution plays an important role in hypothesis testing with linear models.

## Summary

This chapter has reviewed fundamental concepts about random variables, their distributions, expectations, variances, covariances, conditional expectations, and key large‑sample (asymptotic) results, along with several pivotal distributional families (Normal, chi‑squared, $t$, and $F$). It has also motivated why transformations (such as taking logarithms of highly skewed covariates) aid exploratory analysis and subsequent modelling. The material emphasises population (theoretical) quantities that later will be estimated from observed data.

A random variable is described by its distribution: for discrete variables through a probability mass function (pmf) assigning probabilities to admissible values, and for continuous variables through a probability density function (pdf) whose integral over an interval gives the probability of falling within that interval. Expectations provide long‑run average values, while variances capture dispersion. Covariance and correlation quantify linear co‑movement between two random variables. Conditional expectations refine the notion of an average by restricting attention to subsets defined through the value of another random variable and themselves form random variables. Large‑sample theorems (Law of Large Numbers and Central Limit Theorem) formalise why empirical averages stably estimate theoretical means and acquire approximate Normality, enabling inference. Several distributions—Normal, chi‑squared, $t$, and $F$—are structurally linked through sums of squares of Normal variables and ratios thereof, underpinning many classical statistical procedures.


- Random variables and notation:
  - $X$ denotes a random variable; lower‑case symbols like $x$ often denote realised values.
  - Discrete case: pmf $f(k) = P(X = k)$.
  - Continuous case: pdf $f(x)$ with $P(a \le X \le b) = \int_a^b f(x)\,dx$.
  - Cumulative distribution function (cdf): $F(x) = P(X \le x)$.
  - Population mean: $E(X)$; variance: $\operatorname{Var}(X)$; covariance: $\operatorname{Cov}(X,Y)$; correlation: $\operatorname{Corr}(X,Y)$.
  - Bar notation (e.g. $\bar X$) later denotes a sample mean (introduced for large‑sample results).
- Log scaling (motivation illustrated in the population vs per‑capita GMP example):
  - Addresses severe right skew and leverage.
  - Can linearise multiplicative relations.
  - Distinguishes transforming an axis scale versus transforming variables in a model specification.
- Linear regression (as referenced in the example context):
  - A linear form such as $\text{pcgmp} = \beta_0 + \beta_1 \text{pop} + \varepsilon$ or $\text{pcgmp} = \alpha_0 + \alpha_1 \log(\text{pop}) + \varepsilon$ relates a response to a covariate.
  - Interpretation differs under raw versus log‑transformed covariates (e.g., $\alpha_1 \log 2$ gives expected change for a doubling when using the log).
  - (Derivations of estimators are not included here so as not to introduce material absent from the provided text.)
- Expectation:
  - Discrete: $E(X) = \sum_k k f(k)$.
  - Continuous: $E(X) = \int_{-\infty}^{\infty} x f(x)\,dx$.
  - Linearity: $E(aX + bY) = aE(X) + bE(Y)$ (implied, though not separately enumerated).
- Variance, covariance, correlation:
  - Variance: $\operatorname{Var}(X) = E\big[(X - E(X))^2\big] = E(X^2) - (E(X))^2.$
  - Covariance: $\operatorname{Cov}(X,Y) = E(XY) - E(X)E(Y).$
  - Correlation: $\operatorname{Corr}(X,Y) = \frac{\operatorname{Cov}(X,Y)}{\sigma_X \sigma_Y}$ with $-1 \le \operatorname{Corr}(X,Y) \le 1$.
  - Special case: $\operatorname{Cov}(X,X) = \operatorname{Var}(X)$; correlation of a non‑degenerate variable with itself is 1.
- Conditional expectation:
  - Discrete: $E(Y \mid X = x) = \sum_y y f_{Y|X}(y|x).$
  - Continuous: $E(Y \mid X = x) = \int y f_{Y|X}(y|x)\,dy.$
  - Functions: For a function $r(x,y)$, $E(r(X,Y)\mid X = x)$ replaces $y$ by $r(x,y)$ inside the summation or integral.
  - It is a function of $x$; the random variable $E(Y \mid X)$ evaluates to that function at the realised value of $X$.
- Worked conditional example (Uniform nesting):
  - Setup: $X \sim \operatorname{Unif}(0,1)$; conditional on $X = x$, $Y \sim \operatorname{Unif}(x,1)$.
  - Conditional mean: $E(Y \mid X = x) = \frac{x+1}{2}$.
  - Unconditional mean via iteration:
    $E(Y) = E\left( \frac{X+1}{2} \right) = \frac{E(X)+1}{2} = \frac{1/2 + 1}{2} = \frac{3}{4}.$
  - Direct integration:
    $E(Y) = \int_0^1 \int_x^1 y \frac{1}{1-x} \, dy \, dx = \frac{3}{4}.$
- Large sample (asymptotic) theorems:
  - Law of Large Numbers (LLN): For iid $X_i$ with finite mean, $\bar X = \frac{1}{n}\sum_{i=1}^n X_i \xrightarrow{p} E(X)$.
    - Convergence in probability: $\lim_{n \to \infty} P(|\bar X - E(X)| > \epsilon) = 0$ for all $\epsilon > 0$.
    - Importance: Justifies using sample mean as a consistent estimator.
  - Central Limit Theorem (CLT): For iid $X_i$ with finite mean and variance,
    $\sqrt{n}\left( \frac{\bar X - E(X)}{\sqrt{\operatorname{Var}(X)}} \right) \xrightarrow{d} N(0,1).$
    - Provides approximate sampling Normality and variance scaling $\operatorname{Var}(\bar X) = \operatorname{Var}(X)/n$ (as used within the Normal approximation).
    - Importance: Underpins interval estimation and hypothesis testing.
- Normal distribution:
  - Density: $f(x) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2\right),\; -\infty < x < \infty.$
  - Parameters: $E(X)=\mu,\; \operatorname{Var}(X)=\sigma^2$.
  - Standard Normal: $\mu = 0,\; \sigma^2 = 1$.
  - Linear combination condition (Exercise): A linear combination $Y = \sum_{i=1}^n a_i X_i$ is Normal if the vector $(X_1,\dots,X_n)$ is jointly multivariate Normal (independence of Normal marginals is a sufficient special case).
  - Counterexample (mixture) shows marginal Normality alone does not guarantee Normality of all linear combinations.
- Chi‑squared distribution:
  - If $Z_1,\dots,Z_n \sim N(0,1)$ iid, then $X = \sum_{i=1}^n Z_i^2 \sim \chi_n^2$.
  - Relationship to Normal: It is a sum of squared independent standard Normal variables.
- $t$-distribution:
  - If $Y \sim N(0,1)$ and $U \sim \chi_n^2$ independent, then
    $T = \frac{Y}{\sqrt{U/n}} \sim t_n.$
  - Shares central bell shape with heavier tails (more probability in extremes).
- $F$-distribution:
  - If $X \sim \chi_n^2$ and $Y \sim \chi_m^2$ independent, then
    $F = \frac{(X/n)}{(Y/m)} \sim F_{n,m}.$
  - Appears in comparing scaled sums of squares (ratio of two independent Normal‑based variance components).
- Inter‑distribution relationships (within provided definitions):
  - Normal $\rightarrow$ Chi‑squared: Squaring and summing independent standard Normals yields chi‑squared.
  - Chi‑squared + Normal $\rightarrow$ $t$: Ratio of a standard Normal and the square root of an independent scaled chi‑squared.
  - Two chi‑squared $\rightarrow$ $F$: Ratio of independent (chi‑squared divided by degrees of freedom) quantities.
- Importance of chi‑squared, $t$, and $F$ (as signalled by definitions):
  - They arise naturally from transformations of Normal variables and underpin classical inference (e.g., tests involving variance components and ratios), with the $F$-distribution explicitly tied to hypothesis testing contexts in linear modelling settings.
  
---

## Bonus

### 1. Normal Distribution

Real‑world scenario: Manufacturing quality control for a machined part’s diameter.

- Situation: A factory produces metal rods; each rod’s diameter is influenced by many tiny additive effects (tool vibration, thermal expansion, feed rate micro‑variation). By the aggregation of many small independent influences, individual diameters are well modelled as Normal.
- Use: Model a single measurement $X$ as $X \sim N(\mu, \sigma^2)$, where $\mu$ is the process target and $\sigma$ the inherent process variation.
- Goals:
  - Estimate the probability a rod is out of tolerance (e.g. $P(|X-\mu|>0.05)$).
  - Construct control limits for a Shewhart chart when sample size per time point is large (sample means approximately Normal by the Central Limit Theorem).
- Why Normal fits: Additive micro‑effects, symmetry of deviations, and empirical histograms often roughly bell‑shaped.

Other everyday Normal contexts:
- Flight arrival delays aggregated across many small delays.
- Aggregated sensor noise.
- Average daily energy consumption across many independent households (sample mean).

---

### 2. $t$-Distribution

Real‑world scenario: Small‑sample A/B test for a website layout.

- Situation: You run an experiment with a small number of sessions ($n=18$) on a high‑value conversion funnel; you measure time on task (roughly continuous, modest skew).
- Statistical task: Test whether the mean time differs from a historical benchmark $\mu_0$.
- Model: Sample $\bar X$ and sample variance $S^2$ from $n$ observations of (approximately) Normal user times with unknown variance.
- Test statistic:
  $$
  T = \frac{\bar X - \mu_0}{S / \sqrt{n}} \sim t_{n-1} \quad \text{(under the null, assuming approximate Normality)}
  $$
- Why $t$ instead of Normal: Variance $\sigma^2$ is unknown; replacing it by $S^2$ inflates tail uncertainty for small $n$. The $t$-distribution accounts for that extra variability.
- Outcome: Use $t_{n-1}$ critical values or p‑value to decide if observed difference is statistically meaningful.

Other $t$ settings:
- Comparing average fuel efficiency of a new prototype car (few test drives).
- Estimating mean blood pressure change in a pilot medical study with a small cohort.

---

### 3. Chi‑Squared ($\chi^2$) Distribution

Real‑world scenario: Assessing whether observed defect counts match expected category proportions in a warehouse inspection.

- Situation: Products are classified into defect types (scratch, dent, misprint, other). Historical data specify expected proportions. You inspect a batch and record counts across $k$ categories.
- Statistic (goodness‑of‑fit):
  $$
  \chi^2 = \sum_{j=1}^{k} \frac{(O_j - E_j)^2}{E_j}
  $$
  Under the null hypothesis (true proportions as specified) and large enough expected counts, $\chi^2$ approximately follows a chi‑squared distribution with $k-1$ degrees of freedom.
- Use: Compare $\chi^2$ to $\chi^2_{k-1}$ critical values to decide if discrepancy is due to random variation.
- Another variance‑focused use: For a Normally distributed measurement process, a sample variance scaled by the true variance:
  $$
  \frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1}
  $$
  This property underpins confidence intervals for $\sigma^2$.

Other chi‑squared contexts:
- Reliability engineering: Testing if failure counts match a Poisson rate structure (via grouped categories).
- Genetics: Hardy–Weinberg equilibrium tests for genotype frequencies.

---

### 4. $F$-Distribution

Real‑world scenario: Comparing predictive models for house prices with and without a set of location features.

- Situation: You fit a baseline linear regression (model 0) and an expanded model (model 1) adding neighbourhood indicators. You want to test whether added features jointly improve fit.
- Statistic (extra sum of squares test):
  $$
  F = \frac{(\text{SSE}_0 - \text{SSE}_1)/q}{\text{SSE}_1/(n - p_1)} \sim F_{q,\, n-p_1}
  $$
  where:
  - $\text{SSE}_0, \text{SSE}_1$ are residual sums of squares for reduced and full models.
  - $q$ is the number of added parameters.
  - $p_1$ is the total number of parameters (including intercept) in the full model.
  - Under Normal error assumptions and if the reduced model is true, $F$ follows an $F$-distribution.
- Decision: Large $F$ indicates the added block reduces error more than expected by chance.

Root relationship:
- If $X \sim \chi^2_n$ and $Y \sim \chi^2_m$, independent, then:
  $$
  F = \frac{(X/n)}{(Y/m)} \sim F_{n,m}
  $$
  This ratio structure arises naturally in comparing two independent variance estimates (numerator vs denominator).

Other $F$ contexts:
- ANOVA: Testing equality of multiple group means via between‑group vs within‑group mean squares.
- Comparing process variances from two production lines (via ratio of sample variances approximated by $F$).


