---
title: "36-401 Homework 1"
author: "DO NOT USE"
subtitle: Due Friday, September 5 by 5:00pm
output: rmarkdown::github_document
urlcolor: blue
---

***General instructions for all assignments***: 

+ Use this file as the template for your submission. Please write your name at the top of this page in the author section.

+ Throughout, there will be placeholders for your answers/code, marked clearly in **bold**. Please put your answers in *italics* or **bold**, thereby making them easier to see. One exception is math: You don't have to write math in italics/bold, but you should write mathematical answers in LaTeX. Alternatively, you may write mathematical answers by hand; **however, if you do so, you must note in your PDF that you've submitted some answers by hand, and clearly "match" the scanned handwritten pages on your PDF in Gradescope to the corresponding question you're answering.**

+ Be sure to include any code you used to arrive at your answers. Furthermore, be sure to clearly explain how you arrived at your answers (i.e., show your work for each question).

+ Although it's okay to discuss homework problems with other students, all of your homework (code, written answers, etc.) should be only your own. Instances of identical, nearly identical, or copied homework will be considered cheating and plagiarism. Furthermore, you cannot copy, or nearly copy, material from someone else (including an online entity or any other resource). In other words, you must follow rules of academic integrity (as detailed in the syllabus).

+ Questions will sometimes have multiple subparts, often denoted with bullets; please do your best to answer each question. That said, each subpart is denoted with the number of points allocated to that subpart. Feel free to use that as guidance for what questions you should prioritize. Partial credit will be given, so do your best to attempt each question.

+ Late homeworks will not be accepted. If you're not able to complete a homework, it is better to submit a partially complete homework on time than a complete homework at a late time (because the latter will not be accepted). Also remember that there is one homework drop for the class.

# Problem 1: Reading a Short Regression-based News Story (9 points)

For this problem, read the short news article available on Canvas under Files/Homeworks/Homework1, called `wpArticle.pdf`. After reading the article, answer the following questions: 

+ (3pts) What is the main covariate/predictor variable discussed in the news story, and what is the outcome/response?

**The main predictor (covariate) is childhood exposure to secondhand smoke (whether the participant lived in a household with a smoker as a child); the primary outcome is adult death from chronic obstructive pulmonary disease (COPD) among lifelong never‑smokers (i.e., COPD mortality risk).**

+ (3pts) In an experiment, the covariate $X$ is randomly assigned by investigators, but in an observational study $X$ is not randomly assigned. (We will see later in the course that this has important implications for causal inference.) Does the article discuss an experiment or an observational study? Explain your reasoning in one sentence.

**Observational study, because researchers did not (and ethically could not) randomly assign childhood secondhand smoke exposure; they recorded recalled past exposure and then followed participants prospectively.**

+ (3pts) Which use of regression seems to best match the goal of the study discussed in the article: for summarizing data, prediction, or inference? Explain your reasoning in 1-2 sentences.

**Inference: the study estimates and interprets an association (a 31% higher risk of COPD death) to draw aetiological conclusions about the long‑term harm of childhood secondhand smoke exposure, rather than primarily to build a predictive model or merely describe raw data.**

# Problem 2: Expectations, Variances, and Covariances in Linear Regression (53 points)

In class, we've discussed how an outcome $Y$ may not be linear with a covariate $X$, but it may be linear with a nonlinear transformation of $X$. In this problem, we will consider such a situation.

Specifically, assume we have $X \sim \text{Unif}(0, 1)$, and $Y = \exp(X) + \epsilon$, where $\epsilon | X \sim N(0, 1)$. Thus, the outcome $Y$ is linear in $\exp(X)$, rather than $X$. Here, by $\exp(X)$, we mean $\exp(X) = e^X$.

## Part A (6pts)

What is the conditional distribution $Y | X$? Please state a specific distribution, as well as the parameter value(s) for that distribution. Those parameter value(s) may or may not depend on $X$. Please explain in 1-3 sentences how you arrived at your answer.

**$Y|X=x \sim N(\exp{(x)}, 1)$**

By the model definition, $Y = \exp(X) + \epsilon$ with $\epsilon \mid X \sim N(0,1)$ and (conditionally) added to the deterministic term $\exp(X)$. A constant shift of a Normal random variable adds to the mean but does not change the variance, so the conditional distribution is Normal with mean $\exp(x)$ and variance $1$.

## Part B (8pts)

Derive $\mathbb{E}[Y]$. Be sure to show your work for how you arrived at your answer.

**Hint**: As suggested by Part A, this problem has given you information about the conditional distribution $Y | X$. How can you use that information to figure out $\mathbb{E}[Y]$?

$$
\begin{aligned}
\mathbb{E}[Y] &= \mathbb{E}\big[\mathbb{E}[Y \mid X]\big]  \\
&= \mathbb{E}[\exp(X)] && \text{since } Y \mid X \sim N(\exp(X), 1) \\
&= \int_{0}^{1} e^{x} \, dx && \text{because } X \sim \text{Unif}(0,1) \\
&= \left[ e^{x} \right]_{0}^{1} = e - 1.
\end{aligned}
$$

## Part C (9pts)

Derive $\text{Var}(Y)$. Be sure to show your work for how you arrived at your answer.

**Hint**: Again, this problem has given you information about the conditional distribution $Y | X$. How can you use that information to figure out $\text{Var}(Y)$?

$$
\text{Var}(Y) = \mathbb{E}[\text{Var}(Y \mid X)] + \text{Var}(\mathbb{E}[Y \mid X]).
$$

We have $Y \mid X \sim N(\exp(X), 1)$, so $\text{Var}(Y \mid X) = 1$ and $\mathbb{E}[Y \mid X] = \exp(X)$. Therefore:

$$
\text{Var}(Y) = \mathbb{E}[1] + \text{Var}(\exp(X)) = 1 + \text{Var}(\exp(X)).
$$

Compute $\text{Var}(\exp(X))$:

$$
\mathbb{E}[e^{X}] = \int_{0}^{1} e^{x} dx = e - 1,
$$
$$
\mathbb{E}[e^{2X}] = \int_{0}^{1} e^{2x} dx = \frac{e^{2} - 1}{2},
$$

$$
\text{Var}(e^{X}) = \mathbb{E}[e^{2X}] - (\mathbb{E}[e^{X}])^{2} = \frac{e^{2} - 1}{2} - (e - 1)^{2}.
$$

Simplify:

$$
\text{Var}(e^{X}) = \frac{e^{2} - 1}{2} - (e^{2} - 2e + 1)
= \frac{e^{2} - 1 - 2e^{2} + 4e - 2}{2}
= \frac{-e^{2} + 4e - 3}{2}.
$$

Thus:

$$
\text{Var}(Y) = 1 + \frac{-e^{2} + 4e - 3}{2} = \frac{-e^{2} + 4e - 1}{2} = \frac{4e - e^{2} - 1}{2}.
$$

## Part D (9pts)

Derive $\mathbb{E}[XY]$. Be sure to show your work for how you arrived at your answer.

**Hint**: You'll be on the right track if you find that you have to compute the integral $\int_0^1 x e^x dx$. For that integral, it can be useful to remember [integration by parts](https://en.wikipedia.org/wiki/Integration_by_parts), which involves evaluating an integral that can be written as $\int_0^1 u(x) v'(x) dx$ for two functions $u(x)$ and $v(x)$, where $v'(x)$ is the derivative of $v(x)$. Thus, when computing $\int_0^1 x e^x dx$, you should consider either setting $x = u(x)$ and $e^x = v'(x)$, or vice versa (whichever you think is easier).

$$
\mathbb{E}[XY] = \mathbb{E}\big[ X \, \mathbb{E}[Y \mid X] \big] = \mathbb{E}[X e^{X}]
$$
Since \(X \sim \text{Unif}(0,1)\),
$$
\mathbb{E}[X e^{X}] = \int_{0}^{1} x e^{x} \, dx
$$
Integrate by parts: let $u = x$, $dv = e^{x} dx$ so $du = dx$, $v = e^{x}$. Then

$$
\int_{0}^{1} x e^{x} dx = \left. x e^{x} \right|_{0}^{1} - \int_{0}^{1} e^{x} dx = (1\cdot e - 0) - (e - 1) = e - (e - 1) = 1
$$

## Part E (6pts)

As we'll discuss later in the course, the true value of the slope of a linear regression model that regresses $Y$ on $X$ is $\beta_1 = \text{Cov}(X, Y)/\text{Var}(X)$. In this problem, we know that $Y$ is not linear in $X$; but in practice, we wouldn't know this, and may well fit a linear regression model. Thus, it's useful to understand the true value that linear regression model is estimating (in this case, $\beta_1$). For this problem, derive $\beta_1$. Be sure to show your work for how you arrived at your answer.

**Hint**: For this problem, $\mathbb{E}[X] = 1/2$ and $\text{Var}(X) = 1/12$. Thus, this problem boils down to figuring out $\text{Cov}(X, Y)$. Your work from previous parts will likely be very helpful.

$$
\beta_1 = \frac{\text{Cov}(X,Y)}{\text{Var}(X)}
$$

Given $X \sim \text{Unif}(0,1)$, we have $\mathbb{E}[X] = \tfrac{1}{2}$ and $\text{Var}(X) = \tfrac{1}{12}$.

From earlier parts:

$$
\mathbb{E}[Y] = e - 1,\qquad \mathbb{E}[XY] = 1
$$

So

$$
\text{Cov}(X,Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y] = 1 - \frac{1}{2}(e - 1) = 1 - \frac{e - 1}{2} = \frac{3 - e}{2}
$$

Therefore

$$
\beta_1 = \frac{(3 - e)/2}{1/12} = \left(\frac{3 - e}{2}\right)\cdot 12 = 6(3 - e) = 18 - 6e
$$

## Part F (6pts)

As we'll discuss later in the course, the true value of the intercept of a linear regression model that regresses $Y$ on $X$ is $\beta_0 = \mathbb{E}[Y] - \beta_1 \mathbb{E}[X]$. For this part, derive $\beta_0$.

**Hint**: In Part B you should have found $\mathbb{E}[Y]$, and in Part E you should have found $\beta_1$. Meanwhile, $\mathbb{E}[X] = 1/2$. So, this part is just asking you to plug in values you found in previous questions and simplify.

$$
\beta_0 = \mathbb{E}[Y] - \beta_1 \mathbb{E}[X]
$$

From Part B: $\mathbb{E}[Y] = e - 1$  
From Part E: $\beta_1 = 18 - 6e$  
Also $\mathbb{E}[X] = \tfrac{1}{2}$.

Compute:
$$
\beta_0 = (e - 1) - (18 - 6e)\left(\tfrac{1}{2}\right)
= e - 1 - 9 + 3e
= 4e - 10
$$


## Part G (9pts)

Now let's consider the random variable $\exp(X)$. As suggested by Part E, the true value of the slope of a linear regression model that regresses $Y$ on $\exp(X)$ is $\beta^*_1 = \text{Cov}(\exp(X), Y)/\text{Var}(\exp(X))$. For this problem, derive $\beta^*_1$, and be sure to show your work for how you arrived at your answer.

**Hint**: Remember, we are given at the beginning of the problem that, truly, $Y$ is linearly related to $\exp(X)$. Thus, you should be able to guess what $\beta_1^*$ is, and this question is asking you to derive $\beta^*_1$. In your derivation, you will likely come across quantities that you derived in previous questions. You may refer to your past work in your answer to this question; just be sure to clearly communicate what past work you're using (i.e., what work from what part) to arrive at your answer.

Let $Z = \exp(X)$. The true data-generating model is

$$
Y = Z + \epsilon, \qquad \epsilon \mid X \sim N(0,1), \text{ and } \epsilon \text{ is mean-zero with } \epsilon \perp Z.
$$

The population slope when regressing \(Y\) on \(Z\) is

$$
\beta_1^* = \frac{\text{Cov}(Z, Y)}{\text{Var}(Z)}.
$$

Since $Y = Z + \epsilon$,

$$
\text{Cov}(Z, Y) = \text{Cov}(Z, Z + \epsilon) = \text{Var}(Z) + \text{Cov}(Z, \epsilon).
$$

Because $\epsilon$ has conditional mean 0 and is uncorrelated with $Z$,

$$
\text{Cov}(Z, \epsilon) = 0 \quad \Rightarrow \quad \text{Cov}(Z, Y) = \text{Var}(Z).
$$

Thus,

$$
\beta_1^* = \frac{\text{Var}(Z)}{\text{Var}(Z)} = 1.
$$

# Problem 3: Simulating a Data-Generating Process (38 points)

In this problem, we'll again consider the data-generating process in Problem 2, where we have a covariate $X \sim \text{Unif}(0, 1)$ and an outcome $Y = \exp(X) + \epsilon$, where $\epsilon | X \sim N(0, 1)$. **However, you do not need to complete Problem 2 in order to complete this problem.** Instead, we'll focus on using R to simulate the data-generating process from Problem 2, and consider estimators using simulated data.

## Part A (8pts)

For this part, complete the following tasks:

+ Write code below that simulates the above data-generating process involving $X$ and $Y$, for `n = 500` observations. Thus, your code should generate 500 random draws for $X$ and 500 random draws for $Y$, based on the above data-generating process. Furthermore, please write your code in such a way that your draws stay the same every time you Knit your .Rmd file.

**Hint**: If you're not sure how to do the last part---where you make sure your draws stay the same every time you Knit---look back at the R tutorial I posted on Canvas (under Files).

```{r}
# PUT YOUR CODE HERE
set.seed(2025)
n <- 500  # Sample size
X <- runif(n, min = 0, max = 1) # Simulate X ~ Uniform(0,1)
epsilon <- rnorm(n, mean = 0, sd = 1)  # Simulate X ~ Uniform(0,1)
Y <- exp(X) + epsilon  # Generate Y = exp(X) + epsilon

head(data.frame(X = X, Y = Y))
```

+ Consider the population-level quantities $\mathbb{E}[X]$ and $\mathbb{E}[Y]$. What are natural estimators for $\mathbb{E}[X]$ and $\mathbb{E}[Y]$, based on your random draws? Please first write a mathematical equation for each of your estimators in terms of the 500 random draws $(X_1,Y_1),\dots,(X_{500},Y_{500})$. Then, write code below that computes and prints each of your estimators, based on the 500 random draws. Finally, state in words what your estimates are for $\mathbb{E}[X]$ and $\mathbb{E}[Y]$, based on your random draws. (**In all assignments, if you are asked to answer a question, please be sure to do outside of code chunks, even if you think your code and output is self-explanatory.**)

```{r}
mu_X_hat <- mean(X)
mu_Y_hat <- mean(Y)

mu_X_hat; mu_Y_hat
```

$$
\hat{\mu}_X = \frac{1}{500}\sum_{i=1}^{500} X_i, \quad
\hat{\mu}_Y = \frac{1}{500}\sum_{i=1}^{500} Y_i.
$$

Based on the simulated draws, the estimate of $\mathbb{E}[X]$ (which is truly 0.5) will be close to 0.5, and the estimate of $\mathbb{E}[Y]$ (whose true value is $e - 1 \approx 1.718$) will be close to 1.7–1.75. The exact numerical values are those printed by the code above (they will remain the same each Knit due to the fixed seed).

## Part B (8pts)

Consider the population-level quantities:

+ $\text{Var}(X)$

+ $\text{Cov}(X, Y)$

+ $\text{Var}(\exp(X))$

+ $\text{Cov}(\exp(X), Y)$

For each of these quantities, write a mathematical equation of a natural estimator for that quantity, in terms of the 500 random draws $(X_1,Y_1),\dots,(X_{500},Y_{500})$. Then, write code below that computes and prints each of your estimators, based on the 500 random draws. Finally, state in words what your estimates are for each of these quantities, based on your random draws.

```{r}
Z <- exp(X)
X_bar <- mean(X); Y_bar <- mean(Y); Z_bar <- mean(Z)

var_X_hat <- var(X); cov_XY_hat <- cov(X, Y)
var_expX_hat <- var(Z); cov_expX_Y_hat <- cov(Z, Y)

var_X_hat; cov_XY_hat; var_expX_hat; cov_expX_Y_hat
```

**[PUT YOUR ANSWER HERE]**

## Part C (6pts)

As stated in Problem 2, the true slope of the linear regression model that regresses $Y$ on $X$ is

$$\beta_1 = \text{Cov}(X, Y)/\text{Var}(X)$$

whereas the true slope when regressing $Y$ on $\exp(X)$ is

$$\beta_1^* = \text{Cov}(\exp(X), Y)/\text{Var}(\exp(X))$$

Notice that $\beta_1$ and $\beta_1^*$ are in terms of variances and covariances. Meanwhile, in Part B, you should have computed estimates for these variances and covariances. Thus, one idea for estimating $\beta_1$ and $\beta_1^*$ is to simply plug in estimates for the variances and covariances. Given this idea, write code below that computes and prints out estimates for $\beta_1$ and $\beta_1^*$, using your estimates from Part B. Then, state in words what your estimates are for each of these quantities, based on your output.

```{r}
# PUT YOUR CODE HERE
set.seed(2025)
n <- 500

X <- runif(n, 0, 1)
epsilon <- rnorm(n, 0, 1)
Y <- exp(X) + epsilon
Z <- exp(X)   # transformed covariate

# sample (plug-in) estimates of needed moments
var_X_hat        <- var(X)
cov_XY_hat       <- cov(X, Y)
var_expX_hat     <- var(Z)
cov_expX_Y_hat   <- cov(Z, Y)

# Plug-in slope estimates
beta1_hat       <- cov_XY_hat / var_X_hat          # for regression of Y on X
beta1_star_hat  <- cov_expX_Y_hat / var_expX_hat   # for regression of Y on exp(X)

beta1_hat; beta1_star_hat
```

- $\hat{\beta}_1 \approx 2.12$ is the plug‑in estimate of the (population) slope when regressing $Y$ directly on $X$. The true theoretical value (from Problem 2) is $\beta_1 = 18 - 6e \approx 1.6903$, so this sample estimate is somewhat higher than the population value; the discrepancy is due to sampling variability (a single sample of size 500) plus the nonlinearity of the underlying relationship increasing variability in the naive linear approximation.

- $\hat{\beta}_1^{*} \approx 1.26$ is the plug‑in estimate of the slope when regressing $Y$ on $e^{X}$. The true value here (because the data generating model is $Y = e^{X} + \epsilon$ with mean‑zero noise independent of $e^{X}$ is $\beta_1^{*} = 1$. The estimate is reasonably close but noticeably above 1; again this reflects random sampling variation (you could verify by repeating the simulation multiple times and observing the distribution of the estimate cluster around 1).

## Part D (8pts)

We'll again consider estimating $\beta_1$ and $\beta_1^*$ from Part C, but now with linear regression models. For this part, use the `lm()` function to appropriately estimate $\beta_1$ and $\beta_1^*$ via linear regression. After defining your linear regression models, use the `coefficients()` function to print out your estimates for $\beta_1$ and $\beta_1^*$. Then, state in words what your estimates are for each of these quantities, based on your output. Finally, answer the following in 1-2 sentences: How do your estimates for $\beta_1$ and $\beta_1^*$ here compare to your estimates from Part C?

**Hint**: If you're unsure how to use the `lm()` function, you should look at the R tutorial on Canvas (under Files). Meanwhile, by "compare" your estimates, I simply mean state whether the estimates here appear similar or different to the estimates in Part C.

```{r}
# PUT YOUR CODE HERE
set.seed(2025)
n <- 500
X <- runif(n, 0, 1)
epsilon <- rnorm(n, 0, 1)
Y <- exp(X) + epsilon
Z <- exp(X)

beta1_plugin      <- cov(X, Y) / var(X)
beta1_star_plugin <- cov(Z, Y) / var(Z)

fit_X  <- lm(Y ~ X)
fit_Z  <- lm(Y ~ Z)

coefs_X <- coefficients(fit_X)      # (Intercept), X
coefs_Z <- coefficients(fit_Z)      # (Intercept), Z

beta1_lm      <- coefs_X["X"]
beta1_star_lm <- coefs_Z["Z"]

results <- list(
  lm_coefficients_Y_on_X       = coefs_X,
  lm_coefficients_Y_on_expX    = coefs_Z,
  slope_plugin_Y_on_X          = beta1_plugin,
  slope_lm_Y_on_X              = beta1_lm,
  slope_plugin_Y_on_expX       = beta1_star_plugin,
  slope_lm_Y_on_expX           = beta1_star_lm,
  slope_differences = c(
    lm_minus_plugin_beta1      = beta1_lm - beta1_plugin,
    lm_minus_plugin_beta1_star = beta1_star_lm - beta1_star_plugin
  )
)

results
```


- Model $Y \sim X$: intercept $\hat{\beta}_0 = 0.6754722$, slope $\hat{\beta}_1 = 2.1173550$.  
- Model $Y \sim e^{X}$: intercept $\hat{\beta}_0^{*} = -0.4316395$, slope $\hat{\beta}_1^{*} = 1.2588194$.

The lm() slope estimates match the plug‑in (Cov/Var) slope estimates exactly up to machine precision (differences on the order of $10^{-15}$), which is expected because in simple linear regression OLS yields $\hat{\beta}_1 = \widehat{\text{Cov}}(X,Y)/\widehat{\text{Var}}(X)$ using the same unbiased (n−1) denominators as R’s var() and cov().

Comparison to Part C: They are numerically the same (within floating-point rounding). Both slopes are higher than their true population targets ($\beta_1 = 18 - 6e \approx 1.6903$ and $\beta_1^{*} = 1$) due to sampling variability in this single sample of size 500.

## Part E (8pts)

To wrap up this homework, use your 500 random draws from Part A to create a scatterplot with $X$ on the x-axis and $Y$ on the y-axis. Then, add the following two lines to your scatterplot:

+ The estimated regression line for the $Y \sim X$ linear model (i.e., the linear regression model that regresses $Y$ on $X$). This should correspond to $\hat{\beta}_0 + \hat{\beta}_1 X$, where these estimates are computed from `lm()`. Please make this line black on your plot.

+ The true regression function line, which is $\exp(X)$. Please make this line red on your plot.

For this part, you just have to produce the desired plot.

**Hint**: If you're unsure how to make a scatterplot or add lines to it, you should review the R tutorial on Canvas (under Files). When plotting your lines in R, you may have to sort your observations (from decreasing to increasing) in order for your lines to display properly. To do this in R, use the `sort()` function. For example, `sort(x)` would sort the vector `x` from least to greatest.

```{r}
# PUT YOUR CODE HERE
# Part E: Scatterplot with fitted linear regression line and true function

set.seed(2025)
n <- 500
X <- runif(n, 0, 1)
epsilon <- rnorm(n, 0, 1)
Y <- exp(X) + epsilon

# Fit linear model
fit_X <- lm(Y ~ X)
coefs <- coefficients(fit_X)  # intercept and slope

# Create a grid for smooth lines
x_grid <- seq(0, 1, length.out = 400)

# Fitted regression line (linear approximation)
y_hat_line <- coefs[1] + coefs[2] * x_grid

# True regression function
true_line <- exp(x_grid)

# Base scatterplot
plot(X, Y,
     pch = 19, col = rgb(0, 0, 0, 0.45),
     xlab = "X",
     ylab = "Y",
     main = "Scatterplot of Y vs X with Fitted Linear Line and True exp(X) Curve")

# Add fitted linear regression line (black)
lines(x_grid, y_hat_line, col = "black", lwd = 2)

# Add true function exp(X) (red)
lines(x_grid, true_line, col = "red", lwd = 2)

# Add legend
legend("topleft",
       legend = c("Data", "Fitted linear regression", "True exp(X)"),
       col = c(rgb(0,0,0,0.45), "black", "red"),
       pch = c(19, NA, NA),
       lwd = c(NA, 2, 2),
       bty = "n")
```


